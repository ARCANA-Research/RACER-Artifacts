/*
This microkernel convolute K (8-bit 2x2 matrix) with 4096 RGB images,
each image is 8-bit 128x128 matrix

See 8bconv2x2.rc for general setup

*/
/*============================= MAIN PROGRAM =================================*/
/*
  (*) Turn on all 4096 clusters to convolve 4096 images (1 cluster per image)
  (*) RGB conv simply have 3 times number of input data compared to grayscale conv
  (*) Grayscale conv can store 4x8 vectors output in 1 compute unit
  (*) RGB instead only store 3x8 vectors in 1 compute unit (8 per conv layer)
  ->  So 8 x 4 = 32 compute units are needed
  (*) A final additional sum up the result of each conv layer. The addition is
  internal to the compute unit. The following illustrate where data are stored:

  [                             COMPUTE UNIT                                 ]
  [   3rd halfset   ][   2nd halfset   ][   1st halfset   ][   0th halfset   ]

  [    ADD result   ][<- R layer result][<- G layer result][<- B layer result]

  (*) Data shifting from different halfset takes different amount of latency
  (*) The conv output is in Z-format
  (*) The last argument of all compute instruction is a simulated-level argument,
      and it represents an energy multiplicative factor. Because a multiword can
      be broken down into multiple compute engine doing the same thing, the multiplicative
      factor is a quick way to calculate the energy consumption of multiple engines.
      Because all engines operate in parallel, there is not equivalent factor for latency.
  (*) Every pipeline instruction (ADD16 in this case) has its argument
      represented with 'x'. This is because the simulator treat multiple additions
      in the same pipeline as 1 instruction (so that it doesnt have to simulate
      the pipeline). An additional argument is then needed for piplined instruction
      because we need to specify how many additions are there in the pipeline fleet.
      So for a fleet of 3 addition:
        - ADD16 h12 h23 h2
        - ADD16 h12 h12 h28
        - ADD16 h12 h53 h29
      Write it as:
        - ADD16 x x x 3
      The simulator right now only consider performance, so the operands are
      omitted anyway (even for non pipelined instructions)
  (*) Because each layer produce 8 results in 1 compute units, the final addition
      fleet size is 24
  (*) The SHIFT instruction (within compute unit) is not supported by the
      simulator for now. Bulk shifting is complicated to get correctly.
      Instead, hand calculate the cost (latency, energy) for each specific
      shift so that the simulator can add it in final cost (TODO)
  (*) There are 3 shifts before the final addition:
      1st SHIFT:
          - Shift R layer into 3rd halfset (addition engine)
          - Shift G layer into 2nd halfset
          - Shift B layer into 1st halfset
          -> (16 + 15 + 14 + ... + 1) x 3 x 8 = 3264 SHIFT
      2nd SHIFT:
          - Shift G into addition engine (R is already in-place)
          - Shift B into 2nd halfset
          -> (16 + 15 + 14 + ... + 1) x 2 x 8  = 2176 SHIFT
      3rd SHIFT:
          - Shift B into addition engine
          -> (16 + 15 + 14 + ... + 1) x 8 = 1088 SHIFT
   (*) A single 1-bit SHIFT (latch into buffer, latch from buffer) has a
       cost of (taken from device-level simulation of a LATCH operation):
          - Latency = 2 x 5.281 = 10.562 (ns)
          - Energy  = 2 x 1.6384 = 3.2768 (pJ)
       The SHIFT cost can be calculated accordingly
*/

// TODO Change this to a for loop
SETBULK 0 4096 64
SETBULK 1 4096 64
SETBULK 2 4096 64
SETBULK 3 4096 64
SETBULK 4 4096 64
SETBULK 5 4096 64
SETBULK 6 4096 64
SETBULK 7 4096 64
SETBULK 8 4096 64
SETBULK 9 4096 64
SETBULK 10 4096 64
SETBULK 11 4096 64
SETBULK 12 4096 64
SETBULK 13 4096 64
SETBULK 14 4096 64
SETBULK 15 4096 64
SETBULK 16 4096 64
SETBULK 17 4096 64
SETBULK 18 4096 64
SETBULK 19 4096 64
SETBULK 20 4096 64
SETBULK 21 4096 64

// 8 <- 7 6 5 4
MAC16 h0v8 b0v7 b0v0 4
MAC16 h0v8 b0v6 b0v1 4
MAC16 h0v8 b0v5 b0v2 4
MAC16 h0v8 b0v4 b0v3 4

// 13 <- 12 11 10 9
MAC16 h0v13 b0v9 b0v0 4
MAC16 h0v13 b0v10 b0v1 4
MAC16 h0v13 b0v11 b0v2 4
MAC16 h0v13 b0v12 b0v3 4

// 18 <-17 16 15 14
MAC16 h0v18 b0v14 b0v0 4
MAC16 h0v18 b0v15 b0v1 4
MAC16 h0v18 b0v16 b0v2 4
MAC16 h0v18 b0v17 b0v3 4

// 23 <-22 21 20 19
MAC16 h0v23 b0v22 b0v0 4
MAC16 h0v23 b0v21 b0v1 4
MAC16 h0v23 b0v20 b0v2 4
MAC16 h0v23 b0v19 b0v3 4

// 28 <-27 26 25 24
MAC16 h0v23 b0v22 b0v0 4
MAC16 h0v23 b0v21 b0v1 4
MAC16 h0v23 b0v20 b0v2 4
MAC16 h0v23 b0v19 b0v3 4

// 33 <-32 31 30 29
MAC16 h0v23 b0v22 b0v0 4
MAC16 h0v23 b0v21 b0v1 4
MAC16 h0v23 b0v20 b0v2 4
MAC16 h0v23 b0v19 b0v3 4

// 38 <-37 36 35 34
MAC16 h0v23 b0v22 b0v0 4
MAC16 h0v23 b0v21 b0v1 4
MAC16 h0v23 b0v20 b0v2 4
MAC16 h0v23 b0v19 b0v3 4

// 43 <-42 41 40 39
MAC16 h0v23 b0v22 b0v0 4
MAC16 h0v23 b0v21 b0v1 4
MAC16 h0v23 b0v20 b0v2 4
MAC16 h0v23 b0v19 b0v3 4

// Shift R Layer into 3rd halfset (latency val, energy val)
SHIFT 1351.936 43808666.4192

// Shift G Layer into 3rd halfset (latency val, energy val)
SHIFT 1351.936 29205777.6128

// Shift B Layer into 3rd halfset (latency val, energy val)
SHIFT 1351.936 14602888.8064

// Final Addition
ADD16 x x x 24 1

UNSETALL
